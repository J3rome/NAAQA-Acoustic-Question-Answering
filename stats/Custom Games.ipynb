{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic functions -- Run Once\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Set default figure size\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings (This might not be a good idea)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Move up one folder to reach the repo root\n",
    "%cd ..\n",
    "\n",
    "from utils.notebook.generic import full_width_notebook\n",
    "# Set notebook to 100% width\n",
    "full_width_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import parse_args_string, prepare_for_task\n",
    "from utils.notebook.generic import notebook_input_prompt\n",
    "from utils.visualization import show_tagged_scene, get_tagged_scene_table_legend\n",
    "from runner import custom_question_inference\n",
    "\n",
    "data_version_name = \"CLEAR_50k_4_inst_1024_win_50_overlap\"\n",
    "model_restore_path = \"/archive/abdj2702/synced_training/CLEAR_50k_4_inst_1024_win_50_overlap_extractor_parallel_3_block_64_proj_40_epoch_876944_extractor/2020-05-13_22h49\"\n",
    "#model_restore_path = \"output/training/v3_fixed_5k_40_inst_1024_win_50_overlap_hpc-puget-necotis/latest\"\n",
    "\n",
    "data_root_path = \"data\"\n",
    "#config_path = \"config/reduction/original_rnn_256.json\"\n",
    "config_path = f\"{model_restore_path}/config_raw_h5_input.json\"\n",
    "#config_path = \"output/training/v3_fixed_5k_40_inst_1024_win_50_overlap_hpc-puget-necotis/latest/config_conv_input.json\"\n",
    "random_seed = 876944\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "\n",
    "# TODO : Retrieve params from {model_restore_path}\n",
    "# TODO : What happen when processed with resnet ?\n",
    "\n",
    "use_cpu = True\n",
    "\n",
    "# Model weight restore\n",
    "model_weight_path = f\"{model_restore_path}/best/model.pt.tar\"\n",
    "model_dict_file_path = f\"{model_restore_path}/dict.json\"\n",
    "\n",
    "# FIXME : Clear mean & std might be wrong (It is written in the config file)\n",
    "\n",
    "arguments = (\n",
    "    f\"--notebook_model_inference --version_name {data_version_name} --config_path {config_path} --batch_size 4 \"\n",
    "    f\"--random_seed {random_seed} --dict_folder questions \"\n",
    "    f\"--h5_image_input \"\n",
    "    #f\"--raw_img_resize_val {image_size} --no_feature_extractor --normalize_with_clear_stats \"\n",
    "    f\"--film_model_weight_path {model_weight_path} --dict_file_path {model_dict_file_path} {'--use_cpu' if use_cpu else ''}\"\n",
    ")\n",
    "\n",
    "#  \n",
    "            \n",
    "args = parse_args_string(arguments)\n",
    "task_and_more, dataloaders, model_and_more = prepare_for_task(args)\n",
    "task, args, flags, paths, device = task_and_more\n",
    "film_model_config, film_model, optimizer, loss_criterion, scheduler, tensorboard = model_and_more\n",
    "datasets = {set_type:dloader.dataset for set_type, dloader in dataloaders.items()}\n",
    "\n",
    "# Retrieve clear stats\n",
    "import json\n",
    "with open(f\"{data_root_path}/{data_version_name}/clear_stats.json\", 'r') as f:\n",
    "    clear_stats = json.load(f)\n",
    "\n",
    "# Print model summary\n",
    "with open(f\"{model_restore_path}/model_summary.txt\", 'r') as f:\n",
    "    print(f.read())\n",
    "    \n",
    "with open(f\"{model_restore_path}/stats.json\", 'r') as f:\n",
    "    stats = json.load(f)\n",
    "    \n",
    "with open(f\"{model_restore_path}/arguments.json\", 'r') as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "# TODO: Check if exist first\n",
    "with open(f\"{model_restore_path}/test_stats.json\", 'r') as f:\n",
    "    test_stats = json.load(f)\n",
    "\n",
    "print(f\"Model was trained for {len(stats)} epoch on {args['version_name']}\")\n",
    "print(f\"Achieved {float(stats[0]['train_acc'])*100:.2f}% on training set and {float(stats[0]['val_acc'])*100:.2f}% on validation set\")\n",
    "print(f\"Got {float(test_stats['accuracy'])*100:.2f}% on test set ({test_stats['version_name']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve random game\n",
    "set_type = 'train'\n",
    "random_game = datasets[set_type].get_random_game(return_game=True)\n",
    "scene_id = random_game['scene_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show scene & Example questions\n",
    "from utils.notebook.inference import show_game_notebook_input\n",
    "\n",
    "custom_question, custom_questions, legend = show_game_notebook_input(dataloaders[set_type], random_game, clear_stats,\n",
    "                                                                     remove_image_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.notebook.inference import do_custom_question_inference, show_gradcam\n",
    "\n",
    "# MODIFY THIS TO GET GRADCAM FOR DIFFERENT GUESSES\n",
    "gradcam_guess_id = 0\n",
    "\n",
    "custom_game, top_preds = do_custom_question_inference(device, film_model, dataloaders[set_type], custom_question, scene_id, nb_top_pred=5)\n",
    "display(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layers_to_observe = {\n",
    "    #'stem_conv': film_model.stem_conv,\n",
    "    #'resblocks[0].conv1': film_model.resblocks[0].conv1,\n",
    "    #'resblocks[0].conv2': film_model.resblocks[0].conv2,\n",
    "    #'resblocks[0].film_layer': film_model.resblocks[0].film_layer,\n",
    "    #'resblocks[0]': film_model.resblocks[0],\n",
    "    #'resblocks[1].conv1': film_model.resblocks[1].conv1,\n",
    "    #'resblocks[1].conv2': film_model.resblocks[1].conv2,\n",
    "    #'resblocks[1].film_layer': film_model.resblocks[1].film_layer,\n",
    "    #'resblocks[1]': film_model.resblocks[1],\n",
    "    #'resblocks[2].conv1': film_model.resblocks[2].conv1,\n",
    "    #'resblocks[2].conv2': film_model.resblocks[2].conv2,\n",
    "    #'resblocks[2].film_layer': film_model.resblocks[2].film_layer,\n",
    "    #'resblocks[2]': film_model.resblocks[2],\n",
    "    #'resblocks[3].conv1': film_model.resblocks[3].conv1,\n",
    "    #'resblocks[3].conv2': film_model.resblocks[3].conv2,\n",
    "    #'resblocks[3].film_layer': film_model.resblocks[3].film_layer,\n",
    "    #'resblocks[3]': film_model.resblocks[3],\n",
    "    'classifier_conv': film_model.classifier.classif_conv\n",
    "    #'classifier.logits': film_model.classifier.logits\n",
    "}\n",
    "\n",
    "display(legend)\n",
    "heatmaps = show_gradcam(device, film_model, dataloaders[set_type], custom_game, scene_id, guess_id=gradcam_guess_id, \n",
    "                        top_preds=top_preds, clear_stats=clear_stats, apply_relu=True, target_layers=layers_to_observe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_handles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "activations = {}\n",
    "weights = {}\n",
    "biases = {}\n",
    "\n",
    "def forward_hook(name):\n",
    "    def hook(module, inp, output):\n",
    "        activations[name] = output\n",
    "        if isinstance(module, nn.Sequential):\n",
    "            for m in module:\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    weights[name] = m.weight\n",
    "                    \n",
    "                    biases[name] = m.bias\n",
    "        \n",
    "    return hook\n",
    "\n",
    "for layer_name, layer in layers_to_observe.items():\n",
    "    hook_handles.append(layer.register_forward_hook(forward_hook(layer_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hook in hook_handles:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['stem_conv'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations['stem_conv'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    #plt.imshow(npimg, interpolation='nearest')\n",
    "    \n",
    "def separate_features(activations):\n",
    "    bs, c, h, w = activations.size()\n",
    "    assert bs == 1, \"Only work for batch size of 1\"\n",
    "    # Remove batch size\n",
    "    activations = activations.reshape((c,h,w))\n",
    "    activations = activations.div(activations.max())\n",
    "    return list(activations.split(1, dim=0))\n",
    "\n",
    "#show(separate_features(activations['stem_conv'])[0])\n",
    "\n",
    "\n",
    "normalized_activations = activations['classifier.logits'].sub(activations['classifier.logits'].min()).div(activations['classifier.logits'].max().sub(activations['classifier.logits'].min()))\n",
    "separated_feature_maps = separate_features(activations['resblocks[3].film_layer'])\n",
    "show(make_grid(separated_feature_maps[:32], nrow=10))\n",
    "show(make_grid(separated_feature_maps[32:64], nrow=10))\n",
    "show(make_grid(separated_feature_maps[64:96], nrow=10))\n",
    "show(make_grid(separated_feature_maps[96:], nrow=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "separated_feature_maps = separate_features(activations['resblocks[3].conv2'])\n",
    "show(make_grid(separated_feature_maps[:32], nrow=10))\n",
    "show(make_grid(separated_feature_maps[32:64], nrow=10))\n",
    "show(make_grid(separated_feature_maps[64:96], nrow=10))\n",
    "show(make_grid(separated_feature_maps[96:], nrow=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "feature_map = F.upsample(separated_feature_maps[0].unsqueeze(0), size=random_game['image'].size()[1:], mode='bilinear', align_corners=False)\n",
    "\n",
    "heatmap, result = visualize_cam(feature_map, random_game['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show(heatmap)\n",
    "\n",
    "img_min = random_game['image'].min()\n",
    "normalized = random_game['image'].sub(img_min).div(random_game['image'].max() - img_min)\n",
    "\n",
    "heatmaps = []\n",
    "combined = []\n",
    "for feature_map in separated_feature_maps:\n",
    "    upsampled = F.upsample(feature_map.unsqueeze(0), size=random_game['image'].size()[1:], mode='bilinear', align_corners=False)\n",
    "    up_min = upsampled.min()\n",
    "    upsampled = upsampled.sub(up_min).div(upsampled.max() - up_min)\n",
    "    heatmap, result = visualize_cam(upsampled, normalized)\n",
    "    \n",
    "    heatmaps.append(heatmap)\n",
    "    combined.append(result)\n",
    "    \n",
    "show(make_grid(heatmaps))\n",
    "show(make_grid(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heatmaps[28].mean())\n",
    "show(heatmaps[28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(separated_feature_maps[0].squeeze(0).numpy(), interpolation='nearest')\n",
    "\n",
    "\n",
    "#show(separated_feature_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = []\n",
    "\n",
    "for i, feature_map in enumerate(separated_feature_maps):\n",
    "    silence_mean = feature_map[0,:, 10:].sum()\n",
    "    rest_mean = feature_map[0,:, :10].sum()\n",
    "    \n",
    "    if silence_mean > rest_mean:\n",
    "        print(i)\n",
    "        to_plot.append(feature_map)\n",
    "\n",
    "show(make_grid(to_plot, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def visualize_cam(mask, img):\n",
    "    \"\"\" Taken from https://github.com/vickyliin/gradcam_plus_plus-pytorch\n",
    "    Make heatmap from mask and synthesize GradCAM result image using heatmap and img.\n",
    "    Args:\n",
    "        mask (torch.tensor): mask shape of (1, 1, H, W) and each element has value in range [0, 1]\n",
    "        img (torch.tensor): img shape of (1, 3, H, W) and each pixel value is in range [0, 1]\n",
    "\n",
    "    Return:\n",
    "        heatmap (torch.tensor): heatmap img shape of (3, H, W)\n",
    "        result (torch.tensor): synthesized GradCAM result of same shape with heatmap.\n",
    "    \"\"\"\n",
    "    import cv2       # Not an official dependency\n",
    "    heatmap = (255 * mask.squeeze()).type(torch.uint8).cpu().numpy()\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap = torch.from_numpy(heatmap).permute(2, 0, 1).float().div(255)\n",
    "    b, g, r = heatmap.split(1)\n",
    "    heatmap = torch.cat([r, g, b])\n",
    "\n",
    "    result = heatmap*0.2+img.cpu()\n",
    "    min_val = result.min()\n",
    "    result = result.sub(min_val).div(result.max().sub(min_val)).squeeze()\n",
    "\n",
    "    return heatmap, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "film-aqa-torch-1.3",
   "language": "python",
   "name": "film-aqa-torch-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
