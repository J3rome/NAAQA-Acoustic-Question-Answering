{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Magic functions -- Run Once\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "# Move up one folder to reach the repo root\n",
    "%cd ..\n",
    "\n",
    "from utils.notebook.generic import full_width_notebook\n",
    "\n",
    "full_width_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Paths, Imports & Configs\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from utils.notebook.experiment_explorer import get_experiments, get_format_dicts\n",
    "\n",
    "from utils.notebook.pandas import sub_cols_with_cond_and_create_new_col, grouped_scatter, groupby_mean, convert_cols_to_int\n",
    "\n",
    "root_data_path = \"data\"\n",
    "#root_output_path = \"output/training\"\n",
    "root_output_path = \"output_synced/training\"\n",
    "#root_output_path = \"output/synced_training\"\n",
    "\n",
    "# Filtering option\n",
    "latest_experiment_notes = ['extractor_original', 'extractor_original_512_win']\n",
    "\n",
    "# Helpers column selectors\n",
    "run_info_columns = ['nb_sample', 'config', 'nb_epoch_trained', 'stopped_early']\n",
    "more_accuracy_info_columns = ['0.6_at_epoch', '0.7_at_epoch', '0.8_at_epoch', '0.9_at_epoch']\n",
    "accuracy_columns = ['best_val_acc', 'train_acc', 'test_acc']\n",
    "loss_columns = ['best_val_loss', 'train_loss', 'test_loss']\n",
    "all_training_stats = ['all_train_acc', 'all_train_loss', 'all_val_acc', 'all_val_loss']\n",
    "model_config_columns = ['nb_trainable_param', 'word_embedding_dim', 'rnn_state_size', 'extractor_type', \n",
    "                        'stem_out_chan', 'nb_resblock', 'resblocks_out_chan', 'classifier_conv_out_chan', \n",
    "                        'classifier_type', 'classifier_global_pool', 'nb_answer']\n",
    "optimizer_columns = ['optimizer_type', 'optimizer_lr', 'optimizer_weight_decay', 'dropout_drop_prob']\n",
    "preprocessing_columns = ['pad_to_largest', 'resized_height', 'resized_width']\n",
    "spectrogram_columns = ['n_fft', 'hop_length', 'keep_freq_point', 'n_mels', 'resample_audio']\n",
    "normalization_columns = ['norm_zero_one', 'norm_clear_stats']\n",
    "timing_columns = ['train_time', 'mean_epoch_time']\n",
    "other_columns = ['prefix', 'nb_scene', 'nb_q_per_scene', 'date', 'batch_size', 'resnet_features', 'test_version', \n",
    "                 'random_seed', 'nb_non_trainable_param', 'total_nb_param', 'nb_epoch', 'stop_accuracy', 'folder', 'gpu_name']\n",
    "\n",
    "audio_cols = run_info_columns + accuracy_columns + spectrogram_columns + normalization_columns + ['random_seed', 'nb_trainable_param_round', 'note']\n",
    "exp_results_cols = run_info_columns + ['nb_scene', 'nb_q_per_scene'] + accuracy_columns + more_accuracy_info_columns + ['nb_trainable_param_round', 'note']\n",
    "\n",
    "# Retrieve all experimentsi infos\n",
    "experiments = get_experiments(root_output_path, question_type_analysis=False)\n",
    "\n",
    "# Pretty printing\n",
    "format_dict, latex_format_dict = get_format_dicts()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "exp_results = experiments[exp_results_cols].sort_values('test_acc', ascending=False)\n",
    "audio_results = experiments[audio_cols].sort_values('test_acc', ascending=False)\n",
    "\n",
    "latest_experiments = audio_results[audio_results['note'].isin(latest_experiment_notes)]\n",
    "\n",
    "# Print available columns\n",
    "experiments.columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latest = experiments[experiments['date'] >= '2020-09-12'].sort_values('date', ascending=False)\n",
    "latest[audio_cols + ['resized_width', 'resized_height', 'extractor_type', 'date', 'train_time']].style.format(format_dict)\n",
    "\n",
    "l = latest[latest['rnn_state_size'].isin([256,512,2048])]\n",
    "l = l[['date', 'random_seed', 'config', 'note', 'rnn_state_size', 'extractor_type', 'extractor_filters', 'extractor_projection_size', 'nb_resblock', 'resblocks_out_chan', 'classifier_conv_out', 'classifier_projection_out', 'train_acc', 'best_val_acc', 'test_acc']]\n",
    "display(l.sort_values('note', ascending=False).style.format(format_dict))\n",
    "#display(l.style.format(format_dict))\n",
    "l['config'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = experiments[experiments['date'] >= '2020-09-12'].sort_values('date', ascending=False)\n",
    "#latest[latest['nb_epoch_trained'] < 15][['config', 'test_acc','nb_epoch_trained']]\n",
    "latest[latest['note'].str.contains('smaller_2d_extractor')][['config','extractor_type','note']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments[experiments['note'].str.contains('table_4_classifier_topologies_1_worker', na=False)]['train_time'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.notebook.experiment_explorer import get_full_sync_experiment_from_drive_script\n",
    "\n",
    "columns = ['mean_epoch_time', 'config', 'random_seed', 'input_type', 'extractor_type', 'train_acc', 'best_val_acc', 'test_acc', 'all_train_acc', 'date']\n",
    "reproduc = experiments[experiments['note'].str.contains(\"REPRODUC_TEST\", na=False) & (~experiments['input_type'].str.contains('raw_h5'))]\n",
    "\n",
    "reproduc[columns].style.format(latex_format_dict)\n",
    "\n",
    "#print(\"\\n\".join(get_full_sync_experiment_from_drive_script(reproduc, \"output_synced/training\", dryrun=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.notebook.pandas import color_by_multi_attribute\n",
    "#filters = (experiments['resized_height'].notna())\n",
    "#filters = (experiments['extractor_type'].str.contains('ConvLearned'))\n",
    "#filters = (experiments['config'].str.contains('film_original'))\n",
    "#filters = (experiments['nb_trainable_param'] < 800000)\n",
    "#filters = ((experiments['nb_resblock'] == 4) & (experiments['resblocks_out_chan'] == 128))\n",
    "#filters &= ((experiments['n_fft'] == 512))\n",
    "#filters = (experiments['input_type'].str.contains('RGB'))\n",
    "filters = (experiments['date'] >= '2020-09-12')\n",
    "\n",
    "audio_exp = experiments[filters].sort_values('test_acc', ascending=False)\n",
    "#audio_exp = experiments[filters].sort_values('date', ascending=False)\n",
    "\n",
    "cols = ['config', 'n_fft', 'nb_resblock', 'resblocks_out_chan', 'resized_height', 'stem_spatial_location', 'resblock_spatial_location', 'classifier_spatial_location', 'train_acc', 'best_val_acc', 'test_acc', 'extractor_type', 'nb_trainable_param', 'random_seed', 'max_freq', 'spectrogram_rgb', 'batch_size', 'input_type', 'preprocessed_folder_name']\n",
    "color_by_multi_attribute(audio_exp[cols], 'test_acc', ['stem_spatial_location', 'resblock_spatial_location', 'classifier_spatial_location', 'resized_height', 'extractor_type', 'nb_trainable_param'], cmaps=['Reds', 'ocean', 'ocean', 'ocean'], format_dict=latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(experiments.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_dict, latex_format_dict = get_format_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.notebook.pandas import color_by_multi_attribute\n",
    "#audio_exp = experiments[experiments['input_type'] == 'audio'].sort_values('date', ascending=False)\n",
    "filters = (~experiments['stem_spatial_location'].str.contains('Both'))\n",
    "filters &= (~experiments['input_type'].str.contains(\"RGB\"))\n",
    "#filters = (experiments['stem_spatial_location'].apply(lambda x : len(list(x)) <= 1))\n",
    "#filters &= (experiments['resblock_spatial_location'].apply(lambda x : len(list(x)) <= 1))\n",
    "#filters &= (experiments['classifier_spatial_location'].apply(lambda x : len(list(x)) <= 1))\n",
    "#filters |= ((experiments['config'].isin(['extractor_slim_parallel_3_block_64_proj', 'extractor_slim_interleaved_3_block_64_proj'])) & (experiments['n_fft'] == 512) & (experiments['keep_freq_point'] == 256) & (experiments['resample_audio'].isnull()))\n",
    "audio_exp = experiments[filters].sort_values('test_acc', ascending=False)\n",
    "#audio_exp = experiments.sort_values('date', ascending=False)\n",
    "cols = ['config', 'stem_spatial_location', 'resblock_spatial_location', 'classifier_spatial_location', 'train_acc', 'best_val_acc', 'test_acc', 'extractor_type', 'nb_trainable_param', 'random_seed', 'max_freq', 'spectrogram_rgb', 'batch_size', 'input_type', 'preprocessed_folder_name']\n",
    "#audio_exp[cols].style.format(latex_format_dict)\n",
    "color_by_multi_attribute(audio_exp[cols], 'test_acc', ['stem_spatial_location', 'resblock_spatial_location', 'classifier_spatial_location', 'extractor_type'], cmaps=['Reds', 'ocean', 'ocean', 'ocean'], format_dict=latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(audio_exp['stem_spatial_location'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latest = experiments[experiments['date'] >= '2020-09-12'].sort_values('date', ascending=False)\n",
    "latest[audio_cols + ['extractor_type', 'date', 'train_time']].style.format(format_dict)\n",
    "#latest.sort_values('test_acc', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest[latest['note'] == 'extractor_slim'].sort_values('test_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfft_512_exps = latest[latest['n_fft'] == 512].sort_values('test_acc', ascending=False)\n",
    "nfft_512_exps.style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from utils.notebook.pandas import color_row_by_attribute\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from IPython.core.display import display, Markdown\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "def color_row_by_attribute(df, attribute, cmap=None, format_dict=None):\n",
    "    \"\"\"\n",
    "    Will color the rows based on the attribute values.\n",
    "    Data should be sorted in the desired order before passing to this function\n",
    "    This will return a Styler object.\n",
    "    \"\"\"\n",
    "\n",
    "    def styler_fct(sample, attribute, min_max, categorical_values=None, cmap=None):\n",
    "\n",
    "        if cmap is None:\n",
    "            cmap = plt.get_cmap('Blues')\n",
    "\n",
    "        val = sample[attribute]\n",
    "        \n",
    "        if pd.isnull(val):\n",
    "            val = 0\n",
    "        \n",
    "        if categorical_values:\n",
    "            val = categorical_values[val]\n",
    "            \n",
    "        # Will generate a value in the range of 20,220\n",
    "        val = int(((val - min_max[0]) / (min_max[1] - min_max[0])) * 200 + 20)\n",
    "\n",
    "        color = tuple([int(c*255) for i, c in enumerate(cmap(val)) if i < 3])\n",
    "        css = f\"background-color: rgb{color}\"\n",
    "        return [css] * len(sample.index)\n",
    "    \n",
    "    # Should we fill with None ?\n",
    "    df_copy = df.fillna(value=0)\n",
    "    \n",
    "    # What happend when there is a none value in the column ? What about Nn ?\n",
    "    if not is_numeric_dtype(df_copy[attribute]):\n",
    "        # Create range from 0 to 1 for categorical data\n",
    "        unique_values = df_copy[attribute].unique()\n",
    "        nb_values = len(unique_values)\n",
    "        \n",
    "        categorical_values = {value: i/nb_values for i, value in enumerate(unique_values)}\n",
    "        min_max = (0, 1)\n",
    "    else:\n",
    "        min_max = df_copy[attribute].min(), df_copy[attribute].max()\n",
    "        categorical_values = None\n",
    "\n",
    "    styler = df_copy.style.apply(styler_fct, \n",
    "                           attribute=attribute, \n",
    "                           min_max=min_max,\n",
    "                           categorical_values=categorical_values,\n",
    "                           cmap=cmap, \n",
    "                           axis=1)\n",
    "    \n",
    "    if format_dict:\n",
    "        styler = styler.format(format_dict)\n",
    "    \n",
    "    return styler\n",
    "\n",
    "\n",
    "def color_by_multi_attribute(df, main_attribute, attributes=None, cmaps=None, format_dict=None, print_infos=True):\n",
    "    \"\"\"\n",
    "    Will color the whole rows based on the main_attribute values.\n",
    "    Will color specific columns based on attributes values (Over the main_attribute color)\n",
    "    Data should be sorted in the desired order before passing to this function\n",
    "    This will return a Styler object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Color map handling\n",
    "    if cmaps is None:\n",
    "        cmaps = ['Blues']\n",
    "    elif type(cmaps) != list:\n",
    "        cmaps = [cmaps]\n",
    "        \n",
    "    for i, cmap in enumerate(cmaps):\n",
    "        if type(cmap) == str:\n",
    "            cmaps[i] = plt.get_cmap(cmap)\n",
    "            \n",
    "    main_attribute_cmap = cmaps[0]\n",
    "    other_attributes_cmaps = cmaps[1:]\n",
    "\n",
    "    # Styler fct\n",
    "    def main_attribute_styler_fct(sample, attribute, min_max, categorical_values=None, cmap=None):\n",
    "        val = sample[attribute]\n",
    "        \n",
    "        if pd.isnull(val):\n",
    "            val = 0\n",
    "        \n",
    "        if categorical_values:\n",
    "            val = categorical_values[val]\n",
    "            \n",
    "        # Will generate a value in the range of 20,220\n",
    "        val = int(((val - min_max[0]) / (min_max[1] - min_max[0])) * 200 + 20)\n",
    "\n",
    "        color = tuple([int(c*255) for i, c in enumerate(cmap(val)) if i < 3])\n",
    "        css = f\"background-color: rgb{color}\"\n",
    "        return [css] * len(sample.index)\n",
    "    \n",
    "    # Filling NaN values so their value can be expressed as color with cmap\n",
    "    df_copy = df.fillna(value=0)\n",
    "    \n",
    "    # Prepare data normalization\n",
    "    if not is_numeric_dtype(df_copy[main_attribute]):                                   # FIXME : Or nb unique value < X ?\n",
    "        # Create range from 0 to 1 for categorical data\n",
    "        unique_values = df_copy[main_attribute].unique()\n",
    "        nb_values = len(unique_values)\n",
    "        \n",
    "        categorical_values = {value: i/nb_values for i, value in enumerate(unique_values)}\n",
    "        min_max = (0, 1)\n",
    "    else:\n",
    "        min_max = df_copy[main_attribute].min(), df_copy[main_attribute].max()\n",
    "        categorical_values = None\n",
    "\n",
    "    if print_infos:\n",
    "        print(f\"[MAIN] Highlighting '{main_attribute}' with cmap '{main_attribute_cmap.name}'\")\n",
    "        \n",
    "    styler = df_copy.style.apply(main_attribute_styler_fct, \n",
    "                           attribute=main_attribute, \n",
    "                           min_max=min_max,\n",
    "                           categorical_values=categorical_values,\n",
    "                           cmap=main_attribute_cmap, \n",
    "                           axis=1)\n",
    "    \n",
    "    # Highlight specific columns over the main_attributes highlighting\n",
    "    if attributes:\n",
    "        if type(attributes) != list:\n",
    "            attributes = [attributes]\n",
    "        \n",
    "        # We won't use reversed cmaps, if the user want it, he can manually specify it\n",
    "        all_cmaps = [m for m in plt.colormaps() if '_r' not in m]\n",
    "        \n",
    "        # FIXME : Need something like if not is_numeric_dtype(df_copy[main_attribute])\n",
    "        \n",
    "        while len(other_attributes_cmaps) < len(attributes):\n",
    "            new_cmap = plt.get_cmap(random.sample(all_cmaps, 1)[0])\n",
    "            \n",
    "            if new_cmap not in other_attributes_cmaps:\n",
    "                other_attributes_cmaps.append(new_cmap)\n",
    "                    \n",
    "        for attribute, cmap in zip(attributes, other_attributes_cmaps):\n",
    "            styler = styler.background_gradient(cmap=cmap, subset=[attribute])\n",
    "            \n",
    "            if print_infos:\n",
    "                print(f\"[SUB]  Highlighting colum '{attribute}' with cmap '{cmap.name}'\")\n",
    "    \n",
    "    if format_dict:\n",
    "        styler = styler.format(format_dict)\n",
    "    \n",
    "    return styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#exps = audio_exp.sort_values(['n_fft', 'test_acc'], ascending=[False, False])\n",
    "exps = audio_exp.sort_values('test_acc', ascending=False)\n",
    "columns = ['config', 'max_freq', 'test_acc', 'n_fft', 'keep_freq_point', 'hop_length','n_mels', 'resample_audio', 'extractor_type', 'nb_trainable_param_round', 'note']\n",
    "exps = exps[columns]\n",
    "\n",
    "grouped = exps.fillna(0).groupby(['config','max_freq','extractor_type', 'n_mels'], as_index=False).mean().sort_values('test_acc', ascending=False)\n",
    "styler = color_by_multi_attribute(grouped, main_attribute='extractor_type', attributes=['n_mels', 'resample_audio', 'nb_trainable_param_round'], cmaps=['YlOrRd', 'RdBu'], format_dict=format_dict)\n",
    "\n",
    "color_by = 'max_freq'\n",
    "#color_by = 'resample_audio'\n",
    "#color_by = 'n_fft'\n",
    "\n",
    "display(Markdown(f\"# Color coded by {color_by.capitalize()}\"))\n",
    "styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exps = audio_exp.sort_values(['n_fft', 'test_acc'], ascending=[False, False])\n",
    "exps = audio_exp.sort_values('test_acc', ascending=False)\n",
    "columns = ['config', 'max_freq', 'test_acc', 'n_fft', 'keep_freq_point', 'hop_length','n_mels', 'resample_audio', 'extractor_type', 'nb_trainable_param_round', 'note']\n",
    "exps = exps[columns]\n",
    "\n",
    "grouped = exps.fillna(0).groupby(['config','max_freq','extractor_type', 'n_mels'], as_index=False).mean().sort_values('test_acc', ascending=False)\n",
    "styler = color_by_multi_attribute(grouped, main_attribute='n_mels', attributes=['extractor_type', 'resample_audio', 'nb_trainable_param_round'], cmaps=['RdBu', 'YlOrRd'], format_dict=format_dict)\n",
    "\n",
    "color_by = 'max_freq'\n",
    "#color_by = 'resample_audio'\n",
    "#color_by = 'n_fft'\n",
    "\n",
    "display(Markdown(f\"# Color coded by {color_by.capitalize()}\"))\n",
    "styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exps.fillna(0).groupby(['n_fft', 'keep_freq_point', 'resample_audio']).mean().sort_values('test_acc', ascending=False)\n",
    "#exps.fillna(0).groupby(['nb_trainable_param_round', 'resample_audio']).mean().sort_values('test_acc', ascending=False)\n",
    "#exps.fillna(0).groupby(['config', 'resample_audio']).mean().sort_values('test_acc', ascending=False)\n",
    "grouped = exps.fillna(0).groupby(['config','max_freq']).mean().sort_values('test_acc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24000 / (256 // 2) * 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped.style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_by = 'nb_trainable_param_round'\n",
    "display(Markdown(f\"# Color coded by {color_by.capitalize()}\"))\n",
    "color_row_by_attribute(grouped, color_by, cmap=plt.get_cmap('YlOrRd'), format_dict=format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styler = color_row_by_attribute(grouped2, attribute='test_acc', cmap=plt.get_cmap('YlOrRd_r'), format_dict=format_dict)\n",
    "styler = styler.background_gradient(subset=['nb_trainable_param_round'])\n",
    "styler = styler.background_gradient(cmap='BrBG', subset=['max_freq'])\n",
    "\n",
    "styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped2 = exps.fillna(0).groupby(['config','max_freq'], as_index=False).mean().sort_values('test_acc', ascending=False)\n",
    "grouped2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('YlOrRd')\n",
    "cmap = None\n",
    "\n",
    "exps = experiments[experiments['date'] >= '2020-08-17'].sort_values('date', ascending=False)\n",
    "cols = run_info_columns + accuracy_columns + ['extractor_filters', 'extractor_projection_size', 'extractor_out_chan'] + ['n_fft', 'n_mels'] + ['random_seed', 'nb_trainable_param_round', 'note']\n",
    "\n",
    "exps = exps.sort_values('test_acc', ascending=False)[cols]\n",
    "\n",
    "color_by = 'extractor_projection_size'\n",
    "display(Markdown(f\"# Color coded by {color_by.capitalize()}\"))\n",
    "styler = color_row_by_attribute(exps, color_by, cmap=cmap, format_dict=format_dict)\n",
    "\n",
    "exps.style.use(styler.export())\n",
    "#styler.bar(subset=['train_acc'], color='#000000', axis=0, align='zero')\n",
    "\n",
    "t = styler.export()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#color_row_by_attribute(latest, 'n_fft', {512:'red', 4096:'yellow'}).sort_values('test_acc', ascending=False).style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_row_by_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(latest['n_fft'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = plt.get_cmap('Blues')\n",
    "y(200)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_colum_dist(df, colum, data_selectors, legend_title=None):\n",
    "    legend_title='n_fft'\n",
    "\n",
    "    plt.figure()\n",
    "    for selector in data_selectors:    \n",
    "        sns.distplot(df[selector['selector']][colum])\n",
    "    plt.legend(labels=[d['title'] for d in data_selectors], title=legend_title)\n",
    "\n",
    "\n",
    "data_selectors = [\n",
    "    {'title': '512', 'selector': latest['n_fft'] == 512},\n",
    "    {'title': '4096', 'selector': latest['n_fft'] == 4096}\n",
    "]\n",
    "\n",
    "plot_colum_dist(latest, 'test_acc', data_selectors, legend_title='n_fft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kind='hex'\n",
    "kind='kde'\n",
    "kind='scatter'\n",
    "\n",
    "p = sns.jointplot(x='n_fft', y='test_acc', data=latest[latest['n_fft'] == 512], kind='kde', hue='n_fft', n_levels=5, hue_kws={\"cmap\": [\"Blues\", \"Greens\"]})\n",
    "\n",
    "#p.ax_joint.scatter(x='n_fft', y='test_acc', data=latest, c='n_fft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pairplot_cols = ['test_acc', 'n_fft', 'nb_epoch_trained', 'nb_trainable_param_round']\n",
    "\n",
    "sns.pairplot(latest[pairplot_cols], hue='n_fft', kind='scatter')\n",
    "#sns.pairplot(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "sns.violinplot(x='nb_trainable_param_round', y='test_acc', data=latest, hue='n_fft', split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "sns.stripplot(x='n_fft', y='test_acc', data=latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.kdeplot(latest[['n_fft', 'test_acc']], shade=True, n_levels=5, cbar=True)\n",
    "sns.scatterplot(x='n_fft', y='test_acc', data=latest, hue='n_fft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.distplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfft_4096_exps = latest[latest['n_fft'] == 4096].sort_values('test_acc', ascending=False)\n",
    "nfft_4096_exps.style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = experiments[audio_cols + ['date', 'train_time']].sort_values('test_acc', ascending=False)\n",
    "\n",
    "#t = t[t['note'].isin(latest_experiment_notes)]\n",
    "\n",
    "t.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.sort_values('date', ascending=False)[audio_cols + ['train_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiments[experiments['config'].str.contains(\"3_block_64_proj\")].sort_values('test_acc', ascending=False)[exp_results_cols].style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(experiments['note'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#experiments[experiments['note'] == 'batching']['config'].values.tolist()\n",
    "df_filter = (experiments['config'].isin(experiments[experiments['note'] == 'batching']['config'].values.tolist()))\n",
    "df_filter &= (experiments['nb_scene'] == 50000) & (experiments['nb_q_per_scene'] == 4)\n",
    "df_filter &= (experiments['note'] == 'batching') | (experiments['note'] == 'extractor')\n",
    "df_filter |= ((experiments['config']=='reduction_original_rnn_4096') & (experiments['note'] == 'final_dropout'))\n",
    "experiments[df_filter].drop_duplicates(subset=['config', 'nb_scene', 'nb_q_per_scene', 'note'], keep='first').sort_values('config')[exp_results_cols + ['folder']]\n",
    "#experiments[experiments['note'] == 'batching'].sort_values('date', ascending=False)[exp_results_cols].style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delete_experiment_from_drive_script(df, dryrun=False):\n",
    "    for f, d in df[['folder', 'date']].values:\n",
    "        res_path = f\"{f}/{d.strftime('%Y-%m-%d_%Hh%M')}\"\n",
    "        cmd = f\"rclone delete Drive:result/training/{res_path} -P\"\n",
    "        if dryrun:\n",
    "            cmd += \" --dry-run\"\n",
    "        print(cmd)\n",
    "        \n",
    "get_delete_experiment_from_drive_script(experiments[experiments['note'] == 'resnet_noratio'], dryrun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments[((experiments['note'] == 'extractor') & (experiments['extractor_type'] == 'freq_time_interlaced'))].sort_values('test_acc', ascending=False)[exp_results_cols + ['extractor_type']].style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def convert_cols_to_int2(df, columns):\n",
    "    convert_dict = {col : int for col in columns}\n",
    "    # For some reasons, this stopped working.. Can't convert float64 to Nullable Int... Now need to fill NaN value\n",
    "    for col in columns:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "    return df.astype(convert_dict)\n",
    "\n",
    "df = groupby_mean(experiments[(experiments['note'] == 'extractor')], ['config', 'nb_sample'], ['best_val_acc', 'train_acc', 'test_acc', '0.6_at_epoch', '0.7_at_epoch', '0.8_at_epoch'], exp_results_cols, add_count_col=True)\n",
    "\n",
    "df = convert_cols_to_int2(df, ['0.6_at_epoch', '0.7_at_epoch', '0.8_at_epoch'])\n",
    "\n",
    "df.style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available configs :\")\n",
    "set(experiments['config'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(experiments.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_results[(exp_results['note'] == 'extractor')].sort_values(['nb_sample', 'test_acc']).style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_filter = (~exp_results['config'].str.contains('film_original')) & (~exp_results['config'].str.contains('freq_time')) & (~exp_results['config'].str.contains('extractor')) & (~exp_results['config'].str.contains('resblock'))\n",
    "df_filter &= (exp_results['nb_sample'] == 200000) & (~exp_results['config'].str.contains('avg')) & (exp_results['stopped_early'] != 'stop_threshold')\n",
    "experiments[exp_results_cols + ['rnn_state_size']][df_filter].groupby('rnn_state_size').mean().sort_values(['test_acc'])#.style.format(format_dict)\n",
    "#exp_results[df_filter].sort_values(['test_acc']).style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results[(exp_results['stopped_early'] == 'stop_threshold') & (~exp_results['config'].str.contains('extractor'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_results[exp_results['nb_scene'] == 50000]\n",
    "#exp_results[exp_results['nb_sample'] == 200000].sort_values(['nb_scene', 'nb_q_per_scene']).style.format(format_dict)\n",
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_results[(exp_results['nb_scene'] <= 10000) & (exp_results['config'].str.contains('reduction'))].sort_values('test_acc').style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = ((experiments['config'].str.contains('reduction')) & (~experiments['config'].str.contains('stem')) & (~experiments['config'].str.contains('extractor')) & (~experiments['config'].str.contains('resblock')) & (~experiments['config'].str.contains('avg'))) | ((experiments['config'] == 'film_original') & (exp_results['nb_scene'] == 20000) & (exp_results['nb_q_per_scene'] == 20))\n",
    "reduction_experiments = experiments[experiments['config'].str.contains('reduction')][exp_results_cols + timing_columns].sort_values('config')\n",
    "reduction_experiments#[reduction_experiments['stopped_early'] == 'NO']['folder'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results[(exp_results['config'].str.contains('interlaced_smallest'))].sort_values('test_acc', ascending=True).style.format(format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "\n",
    "def plot_training_stats(dataframe, names=None, title=None, colormap=cm.viridis, ax=None):\n",
    "    nb_rows = len(dataframe.index)\n",
    "\n",
    "    colorlist = {key: colors.rgb2hex(colormap(i)) for key, i in\n",
    "                 zip(dataframe.index, np.linspace(0, 0.9, nb_rows))}\n",
    "\n",
    "    if title is None:\n",
    "        title = \"Training Stats\"\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, axs = plt.subplots(2,1)\n",
    "        \n",
    "    fig.suptitle(title)\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[1].set_title(\"Loss\")\n",
    "        \n",
    "    for row in dataframe.itertuples():\n",
    "        label = f\"{row.config} - {row.nb_scene//1000}k_{row.nb_q_per_scene}_q\"\n",
    "        lines = axs[0].plot(row.all_train_acc, label=f\"TRAIN - {label}\")\n",
    "        axs[0].plot(row.all_val_acc, color=lines[0].get_color(), linestyle=\"--\")\n",
    "        \n",
    "        axs[1].plot(row.all_train_loss, color=lines[0].get_color())\n",
    "        axs[1].plot(row.all_val_loss, label=f\"VAL - {label}\", color=lines[0].get_color(), linestyle=\"--\")\n",
    "        \n",
    "    # Draw legend out of the plot\n",
    "    for ax in axs:\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0, box.width * 0.5, box.height])\n",
    "\n",
    "        ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "    #for key, group in dataframe.groupby(group_key):\n",
    "    #    group.plot.scatter(ax=ax, x=x_axis, y=y_axis, c=colorlist[key], label=key, title=title)\n",
    "    \n",
    "with_training_stats = experiments[exp_results_cols + all_training_stats]\n",
    "fig, ax = plot_training_stats(with_training_stats[(with_training_stats['config'].str.contains('separated'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_stats(with_training_stats[with_training_stats['nb_q_per_scene'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of test_acc by (nb_sample, nb_scene)\n",
    "exp_results[(~exp_results['config'].str.contains('film_original')) & (~exp_results['config'].str.contains('extractor'))].hist(column='train_acc', by=['nb_sample', 'nb_scene'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = experiments.sort_values('date', ascending=False)[exp_results_cols]\n",
    "e[e['note'] != 'dataset_size_last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "grouped_scatter(experiments, 'config', 'nb_sample', 'test_acc')\n",
    "\n",
    "#grouped_scatter(experiments[experiments['config'] == 'film_original'], 'nb_q_per_scene', 'nb_scene', 'test_acc')\n",
    "\n",
    "test = experiments.copy()\n",
    "\n",
    "test['perf_delta_orig_vs_small_rnn'] = test[test['config'] == 'film_original']['test_acc'] - test[test['config'] == 'film_original_small_rnn']['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sub_cols_with_cond_and_create_new_col(experiments.copy(), 'perf_delta_orig_vs_small_rnn', 'test_acc', test['config'] == 'film_original_small_rnn', test['config'] == 'film_original', test['config'] == 'film_original_small_rnn')\n",
    "\n",
    "test.sort_values(['nb_sample', 'nb_scene'], ascending=False)[run_info_columns + ['test_acc', 'perf_delta_orig_vs_small_rnn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = experiments[experiments['config'] == 'film_original'][['nb_sample', 'nb_scene', 'nb_q_per_scene', 'train_acc', 'best_val_acc', 'test_acc', '0.8_at_epoch']].sort_values(['nb_sample', 'nb_scene']).fillna(\"None\").to_latex(formatters=latex_format_dict)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network reduction -- GRU units\n",
    "df_filter = (((experiments['note'] == 'final_dropout') | (experiments['note'] == 'final_last')) & (experiments['config'].str.contains('reduction')) & (~experiments['config'].str.contains('proj')) & (~experiments['config'].str.contains('conv')) & (experiments['classifier_type'] == 'fcn'))\n",
    "columns = ['nb_trainable_param', 'rnn_state_size', 'train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned', 'config', 'random_seed']#, 'train_time']\n",
    "#df = groupby_mean(experiments[(experiments['note'] == 'extractor')], ['config', 'nb_sample'], ['best_val_acc', 'train_acc', 'test_acc', '0.6_at_epoch', '0.7_at_epoch', '0.8_at_epoch'], exp_results_cols, add_count_col=True)\n",
    "\n",
    "#df = convert_cols_to_int2(df, ['0.6_at_epoch', '0.7_at_epoch', '0.8_at_epoch'])\n",
    "\n",
    "reduction_experiments = experiments[df_filter][columns]\n",
    "\n",
    "# TODO : Add standard deviation\n",
    "# TODO : Seems like the dataframe is not sorted... Is it because it's a groupby ??\n",
    "reduction_experiments = groupby_mean(reduction_experiments, ['config'], ['best_val_acc', 'train_acc', 'test_acc'], columns, add_count_col=False)\n",
    "reduction_experiments.sort_values('nb_trainable_param', ascending=False)\n",
    "#reduction_experiments = experiments[df_filter][columns].sort_values('nb_trainable_param', ascending=False)\n",
    "#print(reduction_experiments.to_latex(index=False, formatters=latex_format_dict))\n",
    "reduction_experiments.style.format(latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Network reduction -- Classifier\n",
    "df_filter =  (experiments['note'] == 'final') & (experiments['config'].str.contains('reduction'))\n",
    "df_filter &= (~experiments['config'].str.contains('extractor')) & (~experiments['config'].str.contains('proj'))\n",
    "df_filter &= (experiments['rnn_state_size'].isin([4096, 1024, 256]))\n",
    "\n",
    "columns = ['nb_trainable_param', 'rnn_state_size', 'classifier_type', 'classifier_conv_out', 'classifier_projection_out', 'train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_trained']#, 'train_time']\n",
    "# This will drop duplicates and keep the biggest test_acc of the duplicates\n",
    "#reduction_experiments = experiments[df_filter].sort_values('test_acc', ascending=False).drop_duplicates(subset=['config', 'nb_scene', 'nb_q_per_scene'], keep='first')\n",
    "# Sort back via trainable params\n",
    "reduction_experiments = experiments[df_filter]\n",
    "\n",
    "# Cleanup\n",
    "\n",
    "reduction_experiments = reduction_experiments.sort_values('nb_trainable_param', ascending=False)[columns]\n",
    "print(reduction_experiments.to_latex(index=False, formatters=latex_format_dict))\n",
    "#reduction_experiments.style.format(latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network reduction -- Reduction Filters/Nb Resblocks\n",
    "df_filter =  (experiments['note'] == 'final') & (experiments['config'].str.contains('reduction'))\n",
    "df_filter &= (experiments['config'].str.contains('extractor')) & (~experiments['config'].str.contains('proj'))\n",
    "df_filter &= (experiments['rnn_state_size'].isin([4096, 1024, 256]))\n",
    "\n",
    "only_rnn_reduction_filter = (experiments['note'] == 'final') & (experiments['config'].isin(['reduction_original_rnn_1024_fcn_no_conv_hidden_256', \"reduction_original_rnn_1024_fcn_conv_256_hidden_512\"]))\n",
    "\n",
    "df_filter |= only_rnn_reduction_filter\n",
    "\n",
    "columns = ['config', 'nb_trainable_param', 'extractor_out_chan', 'stem_out_chan', 'nb_resblock', 'classifier_conv_out', 'classifier_projection_out', 'train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned']#, 'train_time']\n",
    "# This will drop duplicates and keep the biggest test_acc of the duplicates\n",
    "#reduction_experiments = experiments[df_filter].sort_values('test_acc', ascending=False).drop_duplicates(subset=['config', 'nb_scene', 'nb_q_per_scene'], keep='first')\n",
    "# Sort back via trainable params\n",
    "reduction_experiments = experiments[df_filter]\n",
    "\n",
    "#reduction_experiments = reduction_experiments.sort_values(['rnn_state_size', 'extractor_out_chan', 'stem_out_chan', 'nb_resblock', 'classifier_conv_out', 'classifier_projection_out'], ascending=False)[columns]\n",
    "#reduction_experiments = reduction_experiments.sort_values('nb_trainable_param', ascending=False)[columns]\n",
    "reduction_experiments = reduction_experiments.sort_values('test_acc', ascending=False)[columns]\n",
    "#print(reduction_experiments.to_latex(index=False, formatters=latex_format_dict))\n",
    "reduction_experiments.style.format(latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Extractor -- Parallel Extractor\n",
    "df_filter = (experiments['note'] == 'extractor') & (experiments['extractor_type'] == 'freq_time_separated')\n",
    "\n",
    "columns = ['mean_epoch_time','stem_out_chan', 'config','nb_trainable_param', 'extractor_nb_block', 'extractor_filters', 'extractor_projection_size', 'extractor_out_chan', 'train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned']#, 'train_time']\n",
    "\n",
    "reduction_experiments = experiments[df_filter]\n",
    "\n",
    "# Average Results grouped over ['config', 'nb_scene', 'nb_q_per_scene']\n",
    "reduction_experiments = groupby_mean(reduction_experiments, \n",
    "                                     groupby_columns=['config', 'nb_scene', 'nb_q_per_scene'],\n",
    "                                     mean_columns=['train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned'],\n",
    "                                    selected_columns=columns,\n",
    "                                    add_count_col=False)\n",
    "\n",
    "reduction_experiments = convert_cols_to_int(reduction_experiments, ['nb_epoch_runned'])\n",
    "\n",
    "reduction_experiments = reduction_experiments.sort_values('mean_epoch_time', ascending=False)#[columns]\n",
    "#print(reduction_experiments.to_latex(index=False, formatters=latex_format_dict))\n",
    "reduction_experiments.style.format(latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Extractor -- Interlaced Extractor -- Time First\n",
    "df_filter = (experiments['note'] == 'extractor') & (experiments['extractor_type'] == 'freq_time_interlaced')\n",
    "df_filter &= (experiments['config'].str.contains('timefirst'))\n",
    "\n",
    "columns = ['nb_trainable_param', 'extractor_nb_block', 'extractor_filters', 'extractor_projection_size', 'extractor_out_chan', 'train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned']#, 'train_time']\n",
    "\n",
    "reduction_experiments = experiments[df_filter]\n",
    "\n",
    "#reduction_experiments.loc[:, 'time_first'] = experiments['config'].str.contains('timefirst')\n",
    "\n",
    "# Average Results grouped over ['config', 'nb_scene', 'nb_q_per_scene']\n",
    "reduction_experiments = groupby_mean(reduction_experiments, \n",
    "                                     groupby_columns=['config', 'nb_scene', 'nb_q_per_scene'],\n",
    "                                     mean_columns=['train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned'],\n",
    "                                    selected_columns=columns,\n",
    "                                    add_count_col=False)\n",
    "\n",
    "reduction_experiments = convert_cols_to_int(reduction_experiments, ['nb_epoch_runned'])\n",
    "\n",
    "reduction_experiments = reduction_experiments.sort_values('test_acc', ascending=False)#[columns]\n",
    "#print(reduction_experiments.to_latex(index=False, formatters=latex_format_dict))\n",
    "reduction_experiments.style.format(latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Feature Extractor -- Interlaced Extractor -- Freq First\n",
    "df_filter = (experiments['note'] == 'extractor') & (experiments['extractor_type'] == 'freq_time_interlaced')\n",
    "df_filter &= (~experiments['config'].str.contains('timefirst'))\n",
    "\n",
    "columns = ['nb_trainable_param', 'extractor_nb_block', 'extractor_filters', 'extractor_projection_size', 'extractor_out_chan', 'train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned']#, 'train_time']\n",
    "\n",
    "reduction_experiments = experiments[df_filter]\n",
    "\n",
    "#reduction_experiments.loc[:, 'time_first'] = experiments['config'].str.contains('timefirst')\n",
    "\n",
    "# Average Results grouped over ['config', 'nb_scene', 'nb_q_per_scene']\n",
    "reduction_experiments = groupby_mean(reduction_experiments, \n",
    "                                     groupby_columns=['config', 'nb_scene', 'nb_q_per_scene'],\n",
    "                                     mean_columns=['train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned'],\n",
    "                                    selected_columns=columns,\n",
    "                                    add_count_col=False)\n",
    "\n",
    "reduction_experiments = convert_cols_to_int(reduction_experiments, ['nb_epoch_runned'])\n",
    "\n",
    "reduction_experiments = reduction_experiments.sort_values('nb_trainable_param', ascending=False)#[columns]\n",
    "print(reduction_experiments.to_latex(index=False, formatters=latex_format_dict))\n",
    "#reduction_experiments.style.format(latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Extractor -- Interlaced Extractor -- Both Freq First and Time First\n",
    "df_filter = (experiments['note'] == 'extractor') & (experiments['extractor_type'] == 'freq_time_interlaced')\n",
    "#df_filter &= (~experiments['config'].str.contains('timefirst'))\n",
    "\n",
    "columns = ['nb_trainable_param', 'first', 'extractor_nb_block', 'extractor_filters', 'extractor_projection_size', 'extractor_out_chan', 'train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned']#, 'train_time']\n",
    "\n",
    "reduction_experiments = experiments[df_filter]\n",
    "\n",
    "reduction_experiments.loc[:, 'first'] = experiments['config'].apply(lambda c: 'Time' if 'timefirst' in c else 'Frequency')\n",
    "\n",
    "# Average Results grouped over ['config', 'nb_scene', 'nb_q_per_scene']\n",
    "reduction_experiments = groupby_mean(reduction_experiments, \n",
    "                                     groupby_columns=['config', 'nb_scene', 'nb_q_per_scene'],\n",
    "                                     mean_columns=['train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned'],\n",
    "                                    selected_columns=columns,\n",
    "                                    add_count_col=False)\n",
    "\n",
    "reduction_experiments = convert_cols_to_int(reduction_experiments, ['nb_epoch_runned'])\n",
    "\n",
    "reduction_experiments = reduction_experiments.sort_values(['first', 'nb_trainable_param'], ascending=False)#[columns]\n",
    "print(reduction_experiments.to_latex(index=False, formatters=latex_format_dict))\n",
    "#reduction_experiments.style.format(latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Dataset size comparison -- Mixed -- 100k, 200k, 400k samples\n",
    "import matplotlib\n",
    "\n",
    "fig_name = \"dataset_size_all_samples.pdf\"\n",
    "df_filter = (experiments['note'] == 'dataset_size')\n",
    "columns = ['nb_sample', 'nb_scene', 'nb_q_per_scene', 'test_acc']\n",
    "exp = experiments[df_filter][columns]\n",
    "exp = exp.sort_values('nb_q_per_scene')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "lines = []\n",
    "\n",
    "grouped_by_sample = exp[exp['nb_sample'] == 400000]\n",
    "\n",
    "# Get colorlist\n",
    "group_unique_keys = grouped_by_sample['nb_scene'].unique()\n",
    "colorlist = {key: colors.rgb2hex(matplotlib.cm.gist_rainbow_r(i)) for key, i in\n",
    "             zip(group_unique_keys, np.linspace(0, 0.9, len(group_unique_keys)))}\n",
    "\n",
    "# Plot 400k lines & markers\n",
    "lines += ax.plot(grouped_by_sample['nb_q_per_scene'], grouped_by_sample['test_acc'], linewidth=1, linestyle=':', zorder=1)\n",
    "grouped_scatter(grouped_by_sample, 'nb_scene', 'nb_q_per_scene', 'test_acc', ax = ax, show_label=True, colorlist=colorlist, \n",
    "                label_modifier=lambda n: f\"{int(n/1000)}k scenes  \", additional_params={\"marker\": \",\", \"zorder\":2, \"edgecolor\":lines[0].get_markerfacecolor(), \"linewidth\":1})\n",
    "\n",
    "# Plot 200k lines & markers\n",
    "grouped_by_sample = exp[exp['nb_sample'] == 200000]\n",
    "lines += ax.plot(grouped_by_sample['nb_q_per_scene'], grouped_by_sample['test_acc'], linewidth=1, linestyle=':', zorder=1)\n",
    "grouped_scatter(grouped_by_sample, 'nb_scene', 'nb_q_per_scene', 'test_acc', ax = ax, show_label=False, colorlist=colorlist, additional_params={\"marker\": \",\", \"zorder\":2, \"edgecolor\":lines[1].get_markerfacecolor(), \"linewidth\":1})\n",
    "\n",
    "# Plot 100k lines & markers\n",
    "grouped_by_sample = exp[exp['nb_sample'] == 100000]\n",
    "lines += ax.plot(grouped_by_sample['nb_q_per_scene'], grouped_by_sample['test_acc'], linewidth=1, linestyle=':', zorder=1)\n",
    "grouped_scatter(grouped_by_sample, 'nb_scene', 'nb_q_per_scene', 'test_acc', ax = ax, show_label=False, colorlist=colorlist, additional_params={\"marker\": \",\", \"zorder\":2, \"edgecolor\":lines[2].get_markerfacecolor(), \"linewidth\":1})\n",
    "\n",
    "# Remove marker border from legend\n",
    "for legend_handle in ax.get_legend().legendHandles:\n",
    "    legend_handle.set_linewidths(0)\n",
    "\n",
    "# Add Second legend\n",
    "ax.add_artist(matplotlib.legend.Legend(ax, lines, ['400k samples', '200k samples', '100k samples'], loc='center right'))\n",
    "\n",
    "# Set axis infos\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(exp['nb_q_per_scene'].unique())\n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax.set_xlim([0.9, 43])\n",
    "ax.set_xlabel('Number of question per scene')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "fig.savefig(f\"stats/{fig_name}\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batching -- Pad per batch\n",
    "df_filter = experiments['note'] == 'batching_real'\n",
    "\n",
    "columns = ['config', 'nb_trainable_param', 'extractor_type', 'extractor_nb_block', 'extractor_filters', 'extractor_projection_size', 'extractor_out_chan', 'train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned']#, 'train_time']\n",
    "\n",
    "reduction_experiments = experiments[df_filter]\n",
    "\n",
    "reduction_experiments = reduction_experiments.sort_values('nb_trainable_param', ascending=False)#[columns]\n",
    "#print(reduction_experiments.to_latex(index=False, formatters=latex_format_dict))\n",
    "reduction_experiments[columns].style.format(latex_format_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_used_in_batching_exp = experiments[experiments['note'] == 'batching_real']['config'].values.tolist()\n",
    "df_filter = (experiments['config'].isin(config_used_in_batching_exp))\n",
    "df_filter &= (experiments['nb_scene'] == 50000) & (experiments['nb_q_per_scene'] == 4)\n",
    "#df_filter &= (~pd.isnull(experiments['note']))\n",
    "df_filter &= (experiments['note'].isin(['batching_real', 'final_dropout', 'extractor']))\n",
    "\n",
    "exp = experiments[df_filter]\n",
    "\n",
    "exp = exp.sort_values('test_acc', ascending=False).drop_duplicates(['nb_scene', 'nb_q_per_scene', 'config', 'note'], keep='first')\n",
    "\n",
    "#exp['padding'] = \"Whole set\" if exp['note'] == \"batching_real\" else \"Per batch\"\n",
    "exp['batching'] = exp['note'].apply(lambda x: \"Pad to set\" if x == \"batching_real\" else \"Pad to batch\")\n",
    "\n",
    "exp[exp_results_cols + ['batching']].sort_values('config')\n",
    "\n",
    "#experiments[df_filter][exp_results_cols + ['random_seed']].sort_values('config', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More reduction with freq_time extractor\n",
    "df_filter = (experiments['note'] == 'reduction_extractor')\n",
    "df_filter |= (experiments['config'] == 'reduction_original_rnn_1024_fcn_conv_256_hidden_512_extractor_32_stem_32_resblock_3')\n",
    "\n",
    "\n",
    "columns = ['config', 'nb_trainable_param', 'extractor_type', 'extractor_nb_block', 'extractor_filters', 'extractor_projection_size', 'extractor_out_chan', 'stem_out_chan', 'train_acc', 'best_val_acc', 'test_acc', 'nb_epoch_runned']#, 'train_time']\n",
    "\n",
    "reduction_experiments = experiments[df_filter]\n",
    "\n",
    "reduction_experiments = reduction_experiments.sort_values('nb_trainable_param', ascending=False)#[columns]\n",
    "#print(reduction_experiments.to_latex(index=False, formatters=latex_format_dict))\n",
    "reduction_experiments[columns].style.format(latex_format_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "film-aqa-torch-1.3",
   "language": "python",
   "name": "film-aqa-torch-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
