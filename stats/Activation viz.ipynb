{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic functions -- Run Once\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Set default figure size\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings (This might not be a good idea)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Move up one folder to reach the repo root\n",
    "%cd ..\n",
    "\n",
    "from utils.notebook.generic import full_width_notebook\n",
    "# Set notebook to 100% width\n",
    "full_width_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from main import parse_args_string, prepare_for_task\n",
    "from utils.notebook.generic import notebook_input_prompt\n",
    "from utils.visualization import show_tagged_scene, get_tagged_scene_table_legend\n",
    "from utils.file import read_json\n",
    "from runner import custom_question_inference\n",
    "\n",
    "data_version_name = \"CLEAR_50k_4_inst_audio\"\n",
    "model_folder = \"CLEAR_50k_4_inst_audio_config_extractor_slim_interleaved_3_block_64_proj_40_epoch_876944_extractor_slim/2020-09-09_01h04\"\n",
    "\n",
    "model_restore_path = f\"output_synced/training/{model_folder}\"\n",
    "data_root_path = \"data\"\n",
    "\n",
    "# Read arguments from restored model\n",
    "experiment_arguments = read_json(f\"{model_restore_path}/arguments.json\")\n",
    "preprocessing_arguments = read_json(f\"{model_restore_path}/preprocessed_arguments.json\")\n",
    "preprocessed_folder = experiment_arguments['preprocessed_folder_name']\n",
    "config_path = experiment_arguments['config_path']\n",
    "random_seed = experiment_arguments['random_seed']\n",
    "#random_seed = 876944\n",
    "\n",
    "# Options\n",
    "use_cpu = False\n",
    "do_transforms_online = True\n",
    "use_training_data_stats = False    # If true, will normalize data using stats used to train the restored model\n",
    "normalize_with_clear_stats = False\n",
    "resample_to = None    # 24000\n",
    "spectrogram_n_fft = 512\n",
    "spectrogram_hop_length = 2048\n",
    "spectrogram_keep_point = None #256\n",
    "rgb_spectrogram = False\n",
    "scene_sample_freq = 48000\n",
    "\n",
    "\n",
    "# Model weight restore\n",
    "model_weight_path = f\"{model_restore_path}/best/model.pt.tar\"\n",
    "model_dict_file_path = f\"{model_restore_path}/dict.json\"\n",
    "\n",
    "arguments = (\n",
    "    f\"--notebook_model_inference --version_name {data_version_name} --config_path {config_path} --batch_size 4 \"\n",
    "    f\"--random_seed {random_seed} --dict_folder questions \"\n",
    "    f\"--film_model_weight_path {model_weight_path} --dict_file_path {model_dict_file_path}{' --use_cpu' if use_cpu else ''}\"\n",
    ")\n",
    "\n",
    "if not do_transforms_online:\n",
    "    arguments += f' --h5_image_input --preprocessed_folder_name {preprocessed_folder}'\n",
    "    max_frequency = preprocessed_arguments['resample_to']/2 if preprocessed_arguments['resample_to'] else scene_sample_freq/2\n",
    "    \n",
    "    if preprocessed_arguments['spectrogram_keep_freq_point']:\n",
    "        max_frequency = int(max_frequency * (preprocessed_arguments['spectrogram_keep_freq_point']/preprocessed_arguments['spectrogram_n_fft']))\n",
    "else:\n",
    "    arguments += f' --spectrogram_n_fft {spectrogram_n_fft} --spectrogram_hop_length {spectrogram_hop_length}'\n",
    "    if spectrogram_keep_point:\n",
    "        arguments += f' --spectrogram_keep_freq_point {spectrogram_keep_point}'\n",
    "        \n",
    "    if normalize_with_clear_stats:\n",
    "        arguments += ' --normalize_with_clear_stats'\n",
    "        \n",
    "    if rgb_spectrogram:\n",
    "        arguments += ' --spectrogram_rgb'\n",
    "        \n",
    "    arguments += ' --normalize_zero_one --pad_to_largest_image'\n",
    "    #arguments += ' --pad_to_largest_image'\n",
    "    \n",
    "    preprocessed_folder = \"preprocessed_audio\"\n",
    "    \n",
    "    if resample_to:\n",
    "        preprocessed_folder += f\"_resample_{resample_to}\"\n",
    "    \n",
    "    preprocessed_folder += f\"_win_{spectrogram_n_fft}_hop_{spectrogram_hop_length}\"\n",
    "    \n",
    "    if spectrogram_keep_point:\n",
    "        preprocessed_folder += f'_keep_{spectrogram_keep_point}'\n",
    "        \n",
    "    preprocessed_folder += '_norm_zero_one_norm'\n",
    "    \n",
    "    if normalize_with_clear_stats:\n",
    "        preprocessed_folder += '_clear_stats'\n",
    "        \n",
    "    if rgb_spectrogram:\n",
    "        preprocessed_folder += '_RGB'\n",
    "    \n",
    "    arguments += f' --preprocessed_folder_name {preprocessed_folder}'\n",
    "    \n",
    "    if use_training_data_stats:\n",
    "        arguments += f' --clear_stats_file_path {model_restore_path}/preprocessed_data_stats.json'\n",
    "        \n",
    "    if not use_cpu:\n",
    "        arguments += ' --do_transforms_on_gpu'\n",
    "        \n",
    "    max_frequency = resample_to/2 if resample_to else scene_sample_freq/2\n",
    "    \n",
    "    if spectrogram_keep_point:\n",
    "        max_frequency = int(max_frequency * (spectrogram_keep_point/spectrogram_n_fft))\n",
    "            \n",
    "args = parse_args_string(arguments)\n",
    "task_and_more, dataloaders, model_and_more = prepare_for_task(args)\n",
    "task, args, flags, paths, device = task_and_more\n",
    "film_model_config, film_model, optimizer, loss_criterion, scheduler, tensorboard = model_and_more\n",
    "datasets = {set_type:dloader.dataset for set_type, dloader in dataloaders.items()}\n",
    "\n",
    "# Retrieve clear stats\n",
    "import json\n",
    "if use_training_data_stats:\n",
    "    data_stats_filepath = f\"{model_restore_path}/preprocessed_data_stats.json\"\n",
    "else:\n",
    "    data_stats_filepath = f\"{data_root_path}/{data_version_name}/{preprocessed_folder}/clear_stats.json\"\n",
    "\n",
    "exp_stats = read_json(f\"{model_restore_path}/stats.json\")\n",
    "test_stats = read_json(f\"{model_restore_path}/test_stats.json\")\n",
    "\n",
    "clear_stats = read_json(data_stats_filepath)\n",
    "if 'normalize_zero_one' in arguments:\n",
    "    max_val = clear_stats['max']\n",
    "    min_val = clear_stats['min']\n",
    "\n",
    "    clear_stats = {\n",
    "        'mean': [(m - min_val)/(max_val - min_val) for m in clear_stats['mean']],\n",
    "        'std': [(s - min_val)/(max_val - min_val) for s in clear_stats['std']]\n",
    "    }\n",
    "\n",
    "# Print model summary\n",
    "with open(f\"{model_restore_path}/model_summary.txt\", 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(f\"Model was trained for {len(exp_stats)} epoch on {experiment_arguments['version_name']} -- {preprocessed_folder}\")\n",
    "print(f\"Achieved {float(exp_stats[0]['train_acc'])*100:.2f}% on training set and {float(exp_stats[0]['val_acc'])*100:.2f}% on validation set\")\n",
    "print(f\"Got {float(test_stats['accuracy'])*100:.2f}% on test set ({test_stats['version_name']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve random game\n",
    "set_type = 'train'\n",
    "random_game = datasets[set_type].get_random_game(return_game=True)\n",
    "#random_game = datasets[set_type][6]\n",
    "scene_id = random_game['scene_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = random_game['image'].cpu().squeeze(0)\n",
    "plt.figure()\n",
    "plt.imshow(img,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'].get_min_width_image_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_game['image'].squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap = plt.get_cmap('Blues')\n",
    "colored = cmap(random_game['image'].squeeze(0).cpu(), alpha=None)[:,:,:3]\n",
    "#plt.figure()\n",
    "#plt.imshow(random_game['image'].squeeze(0).cpu())\n",
    "plt.figure()\n",
    "plt.hist(colored[:,:,0].reshape(1,-1))\n",
    "#colored[:,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_game['image'].min(), random_game['image'].max(), random_game['image'].mean(), random_game['image'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show scene & Example questions\n",
    "from utils.notebook.inference import show_game_notebook_input\n",
    "\n",
    "custom_question, custom_questions, legend = show_game_notebook_input(dataloaders[set_type], random_game, clear_stats,\n",
    "                                                                     remove_image_padding=False, max_frequency=max_frequency, fill_rect=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.notebook.inference import do_custom_question_inference, show_gradcam\n",
    "\n",
    "# MODIFY THIS TO GET GRADCAM FOR DIFFERENT GUESSES\n",
    "gradcam_guess_id = 0\n",
    "\n",
    "custom_game, top_preds = do_custom_question_inference(device, film_model, dataloaders[set_type], custom_question, scene_id, nb_top_pred=5)\n",
    "display(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layers_to_observe = {\n",
    "    #'stem_conv': film_model.stem_conv,\n",
    "    #'resblocks[0].conv1': film_model.resblocks[0].conv1,\n",
    "    #'resblocks[0].conv2': film_model.resblocks[0].conv2,\n",
    "    #'resblocks[0].film_layer': film_model.resblocks[0].film_layer,\n",
    "    #'resblocks[0]': film_model.resblocks[0],\n",
    "    #'resblocks[1].conv1': film_model.resblocks[1].conv1,\n",
    "    #'resblocks[1].conv2': film_model.resblocks[1].conv2,\n",
    "    #'resblocks[1].film_layer': film_model.resblocks[1].film_layer,\n",
    "    #'resblocks[1]': film_model.resblocks[1],\n",
    "    #'resblocks[2].conv1': film_model.resblocks[2].conv1,\n",
    "    #'resblocks[2].conv2': film_model.resblocks[2].conv2,\n",
    "    #'resblocks[2].film_layer': film_model.resblocks[2].film_layer,\n",
    "    #'resblocks[2]': film_model.resblocks[2],\n",
    "    #'resblocks[3].conv1': film_model.resblocks[3].conv1,\n",
    "    #'resblocks[3].conv2': film_model.resblocks[3].conv2,\n",
    "    #'resblocks[3].film_layer': film_model.resblocks[3].film_layer,\n",
    "    #'resblocks[3]': film_model.resblocks[3],\n",
    "    'classifier_conv': film_model.classifier.classif_conv\n",
    "    #'classifier.logits': film_model.classifier.logits\n",
    "}\n",
    "\n",
    "display(legend)\n",
    "heatmaps = show_gradcam(device, film_model, dataloaders[set_type], custom_game, scene_id, guess_id=gradcam_guess_id, \n",
    "                        top_preds=top_preds, clear_stats=clear_stats, apply_relu=True, target_layers=layers_to_observe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_handles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "activations = {}\n",
    "weights = {}\n",
    "biases = {}\n",
    "\n",
    "def forward_hook(name):\n",
    "    def hook(module, inp, output):\n",
    "        activations[name] = output\n",
    "        if isinstance(module, nn.Sequential):\n",
    "            for m in module:\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    weights[name] = m.weight\n",
    "                    \n",
    "                    biases[name] = m.bias\n",
    "        \n",
    "    return hook\n",
    "\n",
    "for layer_name, layer in layers_to_observe.items():\n",
    "    hook_handles.append(layer.register_forward_hook(forward_hook(layer_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hook in hook_handles:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['stem_conv'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations['stem_conv'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    #plt.imshow(npimg, interpolation='nearest')\n",
    "    \n",
    "def separate_features(activations):\n",
    "    bs, c, h, w = activations.size()\n",
    "    assert bs == 1, \"Only work for batch size of 1\"\n",
    "    # Remove batch size\n",
    "    activations = activations.reshape((c,h,w))\n",
    "    activations = activations.div(activations.max())\n",
    "    return list(activations.split(1, dim=0))\n",
    "\n",
    "#show(separate_features(activations['stem_conv'])[0])\n",
    "\n",
    "\n",
    "normalized_activations = activations['classifier.logits'].sub(activations['classifier.logits'].min()).div(activations['classifier.logits'].max().sub(activations['classifier.logits'].min()))\n",
    "separated_feature_maps = separate_features(activations['resblocks[3].film_layer'])\n",
    "show(make_grid(separated_feature_maps[:32], nrow=10))\n",
    "show(make_grid(separated_feature_maps[32:64], nrow=10))\n",
    "show(make_grid(separated_feature_maps[64:96], nrow=10))\n",
    "show(make_grid(separated_feature_maps[96:], nrow=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "separated_feature_maps = separate_features(activations['resblocks[3].conv2'])\n",
    "show(make_grid(separated_feature_maps[:32], nrow=10))\n",
    "show(make_grid(separated_feature_maps[32:64], nrow=10))\n",
    "show(make_grid(separated_feature_maps[64:96], nrow=10))\n",
    "show(make_grid(separated_feature_maps[96:], nrow=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "feature_map = F.upsample(separated_feature_maps[0].unsqueeze(0), size=random_game['image'].size()[1:], mode='bilinear', align_corners=False)\n",
    "\n",
    "heatmap, result = visualize_cam(feature_map, random_game['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show(heatmap)\n",
    "\n",
    "img_min = random_game['image'].min()\n",
    "normalized = random_game['image'].sub(img_min).div(random_game['image'].max() - img_min)\n",
    "\n",
    "heatmaps = []\n",
    "combined = []\n",
    "for feature_map in separated_feature_maps:\n",
    "    upsampled = F.upsample(feature_map.unsqueeze(0), size=random_game['image'].size()[1:], mode='bilinear', align_corners=False)\n",
    "    up_min = upsampled.min()\n",
    "    upsampled = upsampled.sub(up_min).div(upsampled.max() - up_min)\n",
    "    heatmap, result = visualize_cam(upsampled, normalized)\n",
    "    \n",
    "    heatmaps.append(heatmap)\n",
    "    combined.append(result)\n",
    "    \n",
    "show(make_grid(heatmaps))\n",
    "show(make_grid(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heatmaps[28].mean())\n",
    "show(heatmaps[28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(separated_feature_maps[0].squeeze(0).numpy(), interpolation='nearest')\n",
    "\n",
    "\n",
    "#show(separated_feature_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = []\n",
    "\n",
    "for i, feature_map in enumerate(separated_feature_maps):\n",
    "    silence_mean = feature_map[0,:, 10:].sum()\n",
    "    rest_mean = feature_map[0,:, :10].sum()\n",
    "    \n",
    "    if silence_mean > rest_mean:\n",
    "        print(i)\n",
    "        to_plot.append(feature_map)\n",
    "\n",
    "show(make_grid(to_plot, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def visualize_cam(mask, img):\n",
    "    \"\"\" Taken from https://github.com/vickyliin/gradcam_plus_plus-pytorch\n",
    "    Make heatmap from mask and synthesize GradCAM result image using heatmap and img.\n",
    "    Args:\n",
    "        mask (torch.tensor): mask shape of (1, 1, H, W) and each element has value in range [0, 1]\n",
    "        img (torch.tensor): img shape of (1, 3, H, W) and each pixel value is in range [0, 1]\n",
    "\n",
    "    Return:\n",
    "        heatmap (torch.tensor): heatmap img shape of (3, H, W)\n",
    "        result (torch.tensor): synthesized GradCAM result of same shape with heatmap.\n",
    "    \"\"\"\n",
    "    import cv2       # Not an official dependency\n",
    "    heatmap = (255 * mask.squeeze()).type(torch.uint8).cpu().numpy()\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap = torch.from_numpy(heatmap).permute(2, 0, 1).float().div(255)\n",
    "    b, g, r = heatmap.split(1)\n",
    "    heatmap = torch.cat([r, g, b])\n",
    "\n",
    "    result = heatmap*0.2+img.cpu()\n",
    "    min_val = result.min()\n",
    "    result = result.sub(min_val).div(result.max().sub(min_val)).squeeze()\n",
    "\n",
    "    return heatmap, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "film-aqa-torch-1.3",
   "language": "python",
   "name": "film-aqa-torch-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
